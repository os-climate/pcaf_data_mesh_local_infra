{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845cdd42-1c6e-4e83-9e4c-42b922068a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import json\n",
    "import osc_ingest_trino as osc\n",
    "import trino\n",
    "from sqlalchemy.engine import create_engine\n",
    "import pandas as pd\n",
    "from openscm_units import unit_registry\n",
    "from pint import set_application_registry, Quantity\n",
    "from pint_pandas import PintArray, PintType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f16c20-032e-4956-8c0b-3a32dbb003b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ureg = unit_registry\n",
    "Q_ = ureg.Quantity\n",
    "ureg.default_format = '~'\n",
    "ureg.define(\"CO2e = CO2 = CO2eq\")\n",
    "ureg.define(\"USD = [currency]\")\n",
    "ureg.define(\"EUR = [currency_EUR]\")\n",
    "ureg.define('Millions=1000000')\n",
    "set_application_registry(ureg)\n",
    "\n",
    "# Load environment variables from credentials.env\n",
    "osc.load_credentials_dotenv()\n",
    "\n",
    "\"\"\"\n",
    "import boto3\n",
    "\n",
    "s3_source = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "source_bucket = s3_source.Bucket(os.environ['S3_LANDING_BUCKET'])\n",
    "\"\"\"\n",
    "\n",
    "hive_catalog = 'osc_datacommons_hive_ingest'\n",
    "hive_schema = 'ingest'\n",
    "hive_bucket = osc.attach_s3_bucket('S3_HIVE')\n",
    "\n",
    "\n",
    "ingest_catalog = 'osc_datacommons_dev'\n",
    "ingest_schema = 'mdt_sandbox'\n",
    "pcaf_table_prefix = ''\n",
    "trino_bucket = osc.attach_s3_bucket(\"S3_DEV\")\n",
    "\n",
    "engine = osc.attach_trino_engine(verbose=True, catalog=ingest_catalog)\n",
    "\n",
    "# make sure schema exists, or table creation below will fail in weird ways\n",
    "qres = osc._do_sql(f\"create schema if not exists {ingest_catalog}.{ingest_schema}\", engine, verbose=True)\n",
    "\n",
    "def requantify_df(df, typemap={}):\n",
    "    units_col = None\n",
    "    columns_not_found = [k for k in typemap.keys() if k not in df.columns]\n",
    "    if columns_not_found:\n",
    "        print(f\"columns {columns_not_found} not found in DataFrame\")\n",
    "        raise ValueError\n",
    "    columns_reversed = reversed(df.columns)\n",
    "    for col in columns_reversed:\n",
    "        if col.endswith(\"_units\"):\n",
    "            if units_col:\n",
    "                print(f\"Column {units_col} follows {col} without intervening value column\")\n",
    "                # We expect _units column to follow a non-units column                                                                                                                                 \n",
    "                raise ValueError\n",
    "            units_col = col\n",
    "            continue\n",
    "        if units_col:\n",
    "            if col + '_units' != units_col:\n",
    "                print(f\"Excpecting column name {col}_units but saw {units_col} instead\")\n",
    "                raise ValueError\n",
    "            if (df[units_col]==df[units_col].iloc[0]).all():\n",
    "                # We can make a PintArray since column is of homogeneous type                                                                                                                          \n",
    "                # ...and if the first valid index matches all, we can take first row as good                                                                                                           \n",
    "                new_col = PintArray(df[col], dtype=f\"pint[{df[units_col].iloc[0]}]\")\n",
    "            else:\n",
    "                # Make a pd.Series of Quantity in a way that does not throw UnitStrippedWarning                                                                                                        \n",
    "                new_col = pd.Series(data=df[col], name=col) * pd.Series(data=df[units_col].map(\n",
    "                    lambda x: typemap.get(col, 'dimensionless') if pd.isna(x) else ureg(x).u), name=col)\n",
    "            if col in typemap.keys():\n",
    "                new_col = new_col.astype(f\"pint[{typemap[col]}]\")\n",
    "            df = df.drop(columns=units_col)\n",
    "            df[col] = new_col\n",
    "            units_col = None\n",
    "        elif col in typemap.keys():\n",
    "            df[col] = df[col].astype(f\"pint[{typemap[col]}]\")\n",
    "    return df\n",
    "\n",
    "# If DF_COL contains Pint quantities (because it is a PintArray or an array of Pint Quantities),\n",
    "# return a two-column dataframe of magnitudes and units.\n",
    "# If DF_COL contains no Pint quanities, return it unchanged.\n",
    "\n",
    "def dequantify_column(df_col: pd.Series):\n",
    "    if type(df_col.values)==PintArray:\n",
    "        return pd.DataFrame({df_col.name: df_col.values.quantity.m,\n",
    "                             df_col.name + \"_units\": str(df_col.values.dtype.units)},\n",
    "                            index=df_col.index)\n",
    "    elif df_col.size==0:\n",
    "        return df_col\n",
    "    elif isinstance(df_col.iloc[0], Quantity):\n",
    "        m, u = list(zip(*df_col.map(lambda x: (np.nan, 'dimensionless') if pd.isna(x) else (x.m, str(x.u)))))\n",
    "        return pd.DataFrame({df_col.name: m, df_col.name + \"_units\": u}, index=df_col.index)\n",
    "    else:\n",
    "        return df_col\n",
    "\n",
    "# Rewrite dataframe DF so that columns containing Pint quantities are represented by a column for the Magnitude and column for the Units.\n",
    "# The magnitude column retains the original column name and the units column is renamed with a _units suffix.\n",
    "def dequantify_df(df):\n",
    "    return pd.concat([dequantify_column(df[col]) for col in df.columns], axis=1)\n",
    "\n",
    "# When reading SQL tables to import into DataFrames, it is up to the user to preserve {COL}, {COL}_units pairings so they can be reconstructed.                                                        \n",
    "# If the user does a naive \"select * from ...\" this happens naturally.                                                                                                                                 \n",
    "# We can give a warning when we see a resulting dataframe that could have, but does not have, unit information properly integrated.  But                                                               \n",
    "# fixing the query on the fly becomes difficult when we consider the fully complexity of parsing and rewriting SQL queries to put the units columns in the correct locations.                          \n",
    "# (i.e., properly in the principal SELECT clause (which can have arbitrarily complex terms), not confused by FROM, WHERE, GROUP BY, ORDER BY, etc.)                                                    \n",
    "\n",
    "def read_quantified_sql (sql, tablename, schemaname, engine, index_col=None):\n",
    "    qres = osc._do_sql(f\"describe {schemaname}.{tablename}\", engine, verbose=False)\n",
    "    # tabledesc will be a list of tuples (column, type, extra, comment)                                                                                                                                \n",
    "    colnames = [x[0] for x in qres]\n",
    "    # read columns normally...this will be missing any unit-related information                                                                                                                        \n",
    "    sql_df = pd.read_sql(sql, engine, index_col)\n",
    "    # if the query requests columns that don't otherwise bring unit information along with them, get that information too                                                                              \n",
    "    extra_unit_columns = [ (i, f\"{col}_units\") for i, col in enumerate(sql_df.columns) if f\"{col}_units\" not in sql_df.columns and f\"{col}_units\" in colnames ]\n",
    "    if extra_unit_columns:\n",
    "        extra_unit_columns_positions = [ (i, extra_unit_columns[i][0], extra_unit_columns[i][1]) for i in range(len(extra_unit_columns)) ]\n",
    "        for col_tuple in extra_unit_columns_positions:\n",
    "            print(f\"Missing units column '{col_tuple[2]}' after original column '{sql_df.columns[col_tuple[1]]}' (should be column #{col_tuple[0]+col_tuple[1]+1} in new query)\")\n",
    "        raise ValueError\n",
    "    else:\n",
    "        return requantify_df(sql_df).convert_dtypes()\n",
    "\n",
    "HOMEDIR = os.getcwd().rsplit('/', 1)[0]\n",
    "DBT_DIR = 'pcaf_transform'\n",
    "\n",
    "# The following text describes DBT model properties\n",
    "\n",
    "'''\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - [name](model_name): <model name>\n",
    "    [description](description): <markdown_string>\n",
    "    [docs](resource-properties/docs):\n",
    "      show: true | false\n",
    "    [config](resource-properties/config):\n",
    "      [<model_config>](model-configs): <config_value>\n",
    "    [tests](resource-properties/tests):\n",
    "      - <test>\n",
    "      - ... # declare additional tests\n",
    "    columns:\n",
    "      - name: <column_name> # required\n",
    "        [description](description): <markdown_string>\n",
    "        [meta](meta): {<dictionary>}\n",
    "        [quote](quote): true | false\n",
    "        [tests](resource-properties/tests):\n",
    "          - <test>\n",
    "          - ... # declare additional tests\n",
    "        [tags](resource-configs/tags): [<string>]\n",
    "\n",
    "      - name: ... # declare properties of additional columns\n",
    "'''\n",
    "\n",
    "# The following text describes DBT external properties\n",
    "\n",
    "'''\n",
    "version: 2\n",
    "\n",
    "sources:\n",
    "  - name: <source_name>\n",
    "    tables:\n",
    "      - name: <table_name>\n",
    "        external:\n",
    "          location: <string>\n",
    "          file_format: <string>\n",
    "          row_format: <string>\n",
    "          tbl_properties: <string>      \n",
    "          partitions:\n",
    "            - name: <column_name>\n",
    "              data_type: <string>\n",
    "              description: <string>\n",
    "              meta: {dictionary}\n",
    "            - ...\n",
    "          <additional_property>: <additional_value>\n",
    "'''\n",
    "\n",
    "dbt_dict = {}\n",
    "dbt_dict['models'] = {}\n",
    "\n",
    "def create_trino_table_and_dbt_metadata(tablename, df, partition_columns=[], custom_meta_content='', custom_meta_fields='', verbose=False):\n",
    "    ingest_table = f'{pcaf_table_prefix}{tablename}'\n",
    "\n",
    "    if custom_meta_content:\n",
    "        dbt_models = dbt_dict['models']\n",
    "        dbt_models[ingest_table] = dbt_table = { 'description': custom_meta_content['description']}\n",
    "        if custom_meta_fields:\n",
    "            dbt_table['columns'] = dbt_columns = (\n",
    "                { name: {'description': custom_meta_fields[name]['Description'] } for name in custom_meta_fields.keys() }\n",
    "            )\n",
    "            for name in custom_meta_fields.keys():\n",
    "                if 'tags' in custom_meta_fields[name].keys():\n",
    "                    dbt_columns[name]['tags'] = custom_meta_fields[name]['tags']\n",
    "    elif custom_meta_fields:\n",
    "        raise ValueError\n",
    "\n",
    "    drop_table = osc._do_sql(f\"drop table if exists {ingest_schema}.{ingest_table}_source\", engine, verbose = verbose)\n",
    "\n",
    "    osc.fast_pandas_ingest_via_hive(\n",
    "        df,\n",
    "        engine,\n",
    "        ingest_catalog, ingest_schema, f\"{ingest_table}_source\",\n",
    "        hive_bucket, hive_catalog, hive_schema,\n",
    "        partition_columns = partition_columns,\n",
    "        overwrite = True,\n",
    "        typemap={'datetime64[ns]':'date'},\n",
    "        verbose = verbose\n",
    "    )\n",
    "\n",
    "    with open(f\"{HOMEDIR}/dbt/{DBT_DIR}/models/{ingest_table}.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "        print(\"{{ config(materialized='view', view_security='invoker') }}\" + f\"\"\"\n",
    "with source_data as (\n",
    "    select {', '.join(df.columns)}\n",
    "    from {ingest_catalog}.{ingest_schema}.{ingest_table}_source\n",
    ")\n",
    "select * from source_data\n",
    "\n",
    "\"\"\", file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
